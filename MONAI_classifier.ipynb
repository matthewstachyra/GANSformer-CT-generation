{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb64520",
   "metadata": {},
   "source": [
    "# MONAI (Medical Open Network for AI)\n",
    "\n",
    "MONAI: https://github.com/Project-MONAI\n",
    "\n",
    "Tutorials referenced: https://github.com/Project-MONAI/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd16391f",
   "metadata": {},
   "source": [
    "**Task** Get performance of specialized medical imaging classifer on GANSformer generated CTs versus real CTs.\n",
    "\n",
    "**Expectations**\n",
    "- We expect GANSformer images to \"fool\" MONAI.\n",
    "- Good performance will show as poor performance from the MONAI classifier.\n",
    "- Therefore, we expect MONAI to have a no-better-than-chance AUC classifying against GANSformer.\n",
    "\n",
    "**Results**\n",
    "- Indeed, MONAI performs no better than chance as measured by AUC and precision.\n",
    "- We can conclude that GANSformer images are close enough to the real thing to fool a medical imaging classifier like MONAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "85901f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.8.0\n",
      "Numpy version: 1.21.1\n",
      "Pytorch version: 1.8.0.post3\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "MONAI rev id: 714d00dffe6653e21260160666c4c201ab66511b\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Pillow version: 8.3.1\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.10.0a0\n",
      "tqdm version: 4.62.3\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pandas version: 1.3.4\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import PIL\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.io import read_image\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import decollate_batch\n",
    "from monai.metrics import ROCAUCMetric\n",
    "from monai.networks.nets import DenseNet121\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AddChannel,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    RandFlip,\n",
    "    RandRotate,\n",
    "    RandZoom,\n",
    "    ScaleIntensity,\n",
    "    EnsureType,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0256cb66",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "12487 images\n",
    "\n",
    "11487 are real images from QURE.ai dataset (http://headctstudy.qure.ai/dataset)\n",
    "\n",
    "1000 are GANsformer generated\n",
    "\n",
    "\n",
    "- pre-process fake and real images to be the same dimensions (1,256,256)\n",
    "- combine into one directory\n",
    "- split into train, test, split\n",
    "- create dataset and loaders with batch size 64 using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "d6586b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"data/cts\"\n",
    "paths = os.listdir(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "18bfb9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12487\n",
      "11487\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(paths)) \n",
    "print(len([path for path in paths if \"Sample\" not in path]))\n",
    "print(len([path for path in paths if \"Sample\" in path])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "37950b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# real images are (512, 512)\n",
    "img_path = paths[0]\n",
    "image = PIL.Image.open(img_dir + \"/\" + img_path)\n",
    "image = np.array(image)\n",
    "image.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "19e54fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 3)"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GANSformer generated images are (256, 256, 3)\n",
    "img_path = paths[12486]\n",
    "image = PIL.Image.open(img_dir + \"/\" + img_path)\n",
    "image = np.array(image)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "ad587c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process all to be (1, 256, 256) to match pytorch model input dimensions\n",
    "transform = transforms.Compose([transforms.Scale((256,256))])\n",
    "for n, path in enumerate(paths):\n",
    "    image = PIL.Image.open(img_dir + \"/\" + path) \n",
    "    image = image.convert(\"L\") # remove color channels / convert to greyscale\n",
    "    image = transform(image) # convert to (256,256)\n",
    "    if \".\" not in path:\n",
    "        if \"Sample\" in path:\n",
    "            name = 'data/transformed_cts/generated_image'\n",
    "        else:\n",
    "            name = 'data/transformed_cts/image'\n",
    "    image.save(name + str(n+1) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "6d83b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_img_dir = 'data/transformed_cts'\n",
    "updated_paths = os.listdir(updated_img_dir)\n",
    "updated_paths = [path for path in updated_paths if '.png' in path]\n",
    "img_files = [updated_img_dir+\"/\"+image for image in updated_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "ae0922b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/transformed_cts/generated_image4221.png',\n",
       " 'data/transformed_cts/generated_image2888.png',\n",
       " 'data/transformed_cts/image10104.png',\n",
       " 'data/transformed_cts/image9652.png',\n",
       " 'data/transformed_cts/generated_image2650.png',\n",
       " 'data/transformed_cts/generated_image9705.png',\n",
       " 'data/transformed_cts/image3419.png',\n",
       " 'data/transformed_cts/generated_image6436.png',\n",
       " 'data/transformed_cts/generated_image1359.png',\n",
       " 'data/transformed_cts/image4376.png']"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "7fc8c39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training count: 9991, Validation count: 1248, Test count: 1248\n"
     ]
    }
   ],
   "source": [
    "# define train, validation, test split\n",
    "val_frac = 0.1\n",
    "test_frac = 0.1\n",
    "length = len(paths)\n",
    "indices = np.arange(length)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "test_split = int(test_frac * length)\n",
    "val_split = int(val_frac * length) + test_split\n",
    "test_indices = indices[:test_split]\n",
    "val_indices = indices[test_split:val_split]\n",
    "train_indices = indices[val_split:]\n",
    "\n",
    "img_labels = [0 if \"generated\" in path else 1 for path in updated_paths]\n",
    "\n",
    "train_x = [img_files[i] for i in train_indices]\n",
    "train_y = [img_labels[i] for i in train_indices]\n",
    "val_x = [img_files[i] for i in val_indices]\n",
    "val_y = [img_labels[i] for i in val_indices]\n",
    "test_x = [img_files[i] for i in test_indices]\n",
    "test_y = [img_labels[i] for i in test_indices]\n",
    "\n",
    "print(\n",
    "    f\"Training count: {len(train_x)}, Validation count: \"\n",
    "    f\"{len(val_x)}, Test count: {len(test_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "86ed6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "class CT_Dataset(Dataset):\n",
    "    def __init__(self, image_files, labels, transforms):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.transforms(self.image_files[index]), self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "600a4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms for training and validation sets\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImage(image_only=True),\n",
    "        AddChannel(),\n",
    "        ScaleIntensity(),\n",
    "        RandRotate(range_x=np.pi / 12, prob=0.5, keep_size=True),\n",
    "        RandFlip(spatial_axis=0, prob=0.5),\n",
    "        RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
    "        EnsureType(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [LoadImage(image_only=True), AddChannel(), ScaleIntensity(), EnsureType()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "1c7098ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datasets for training, validation, and testing\n",
    "training = CT_Dataset(train_x, train_y, transforms=train_transforms)\n",
    "validating = CT_Dataset(val_x, val_y, transforms=val_transforms)\n",
    "testing = CT_Dataset(test_x, test_y, transforms=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "33dd2341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check images correctly loaded\n",
    "training.__getitem__(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "df0596b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256])"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check images have correct shape (should be [1, 256, 256] so that with batch it becomes [64, 1, 256, 256])\n",
    "training.__getitem__(50)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "276fba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(training, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(validating, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(testing, batch_size=64, shuffle=True)\n",
    "\n",
    "y_pred_trans = Compose([EnsureType(), Activations(softmax=True)])\n",
    "y_trans = Compose([EnsureType(), AsDiscrete(to_onehot=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95dbe2b",
   "metadata": {},
   "source": [
    "## Train MONAI classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb14028",
   "metadata": {},
   "source": [
    "#### Testing AUC for GANSformer images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "8c4f3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "c5ba4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using DenseNet121\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DenseNet121(spatial_dims=2, in_channels=1,\n",
    "                    out_channels=2).to(device)\n",
    "\n",
    "# cross entropy loss\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n",
    "\n",
    "# AUC\n",
    "auc_metric = ROCAUCMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "df8abcca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/4\n",
      "1/156, train_loss: 0.6816\n",
      "2/156, train_loss: 0.7022\n",
      "3/156, train_loss: 0.6993\n",
      "4/156, train_loss: 0.6809\n",
      "5/156, train_loss: 0.7030\n",
      "6/156, train_loss: 0.6839\n",
      "7/156, train_loss: 0.7014\n",
      "8/156, train_loss: 0.6871\n",
      "9/156, train_loss: 0.6905\n",
      "10/156, train_loss: 0.7036\n",
      "11/156, train_loss: 0.6992\n",
      "12/156, train_loss: 0.6893\n",
      "13/156, train_loss: 0.6925\n",
      "14/156, train_loss: 0.6877\n",
      "15/156, train_loss: 0.6893\n",
      "16/156, train_loss: 0.7014\n",
      "17/156, train_loss: 0.6962\n",
      "18/156, train_loss: 0.6936\n",
      "19/156, train_loss: 0.7006\n",
      "20/156, train_loss: 0.6953\n",
      "21/156, train_loss: 0.6899\n",
      "22/156, train_loss: 0.6956\n",
      "23/156, train_loss: 0.6908\n",
      "24/156, train_loss: 0.6910\n",
      "25/156, train_loss: 0.6901\n",
      "26/156, train_loss: 0.6933\n",
      "27/156, train_loss: 0.6944\n",
      "28/156, train_loss: 0.6928\n",
      "29/156, train_loss: 0.6942\n",
      "30/156, train_loss: 0.6891\n",
      "31/156, train_loss: 0.6897\n",
      "32/156, train_loss: 0.6916\n",
      "33/156, train_loss: 0.6872\n",
      "34/156, train_loss: 0.6877\n",
      "35/156, train_loss: 0.6961\n",
      "36/156, train_loss: 0.6955\n",
      "37/156, train_loss: 0.6854\n",
      "38/156, train_loss: 0.6924\n",
      "39/156, train_loss: 0.6770\n",
      "40/156, train_loss: 0.6876\n",
      "41/156, train_loss: 0.6891\n",
      "42/156, train_loss: 0.6777\n",
      "43/156, train_loss: 0.6765\n",
      "44/156, train_loss: 0.7009\n",
      "45/156, train_loss: 0.7070\n",
      "46/156, train_loss: 0.6806\n",
      "47/156, train_loss: 0.7015\n",
      "48/156, train_loss: 0.6802\n",
      "49/156, train_loss: 0.7052\n",
      "50/156, train_loss: 0.7174\n",
      "51/156, train_loss: 0.7016\n",
      "52/156, train_loss: 0.6975\n",
      "53/156, train_loss: 0.6995\n",
      "54/156, train_loss: 0.6889\n",
      "55/156, train_loss: 0.6864\n",
      "56/156, train_loss: 0.6827\n",
      "57/156, train_loss: 0.7008\n",
      "58/156, train_loss: 0.6938\n",
      "59/156, train_loss: 0.6939\n",
      "60/156, train_loss: 0.7034\n",
      "61/156, train_loss: 0.6868\n",
      "62/156, train_loss: 0.7098\n",
      "63/156, train_loss: 0.6889\n",
      "64/156, train_loss: 0.6971\n",
      "65/156, train_loss: 0.6968\n",
      "66/156, train_loss: 0.6879\n",
      "67/156, train_loss: 0.6947\n",
      "68/156, train_loss: 0.7027\n",
      "69/156, train_loss: 0.6926\n",
      "70/156, train_loss: 0.6895\n",
      "71/156, train_loss: 0.6987\n",
      "72/156, train_loss: 0.6927\n",
      "73/156, train_loss: 0.6826\n",
      "74/156, train_loss: 0.6854\n",
      "75/156, train_loss: 0.6859\n",
      "76/156, train_loss: 0.6963\n",
      "77/156, train_loss: 0.6896\n",
      "78/156, train_loss: 0.6946\n",
      "79/156, train_loss: 0.6861\n",
      "80/156, train_loss: 0.6907\n",
      "81/156, train_loss: 0.6870\n",
      "82/156, train_loss: 0.6953\n",
      "83/156, train_loss: 0.6898\n",
      "84/156, train_loss: 0.6847\n",
      "85/156, train_loss: 0.7048\n",
      "86/156, train_loss: 0.6961\n",
      "87/156, train_loss: 0.6922\n",
      "88/156, train_loss: 0.7003\n",
      "89/156, train_loss: 0.6969\n",
      "90/156, train_loss: 0.6857\n",
      "91/156, train_loss: 0.6851\n",
      "92/156, train_loss: 0.6978\n",
      "93/156, train_loss: 0.6937\n",
      "94/156, train_loss: 0.6982\n",
      "95/156, train_loss: 0.6964\n",
      "96/156, train_loss: 0.7051\n",
      "97/156, train_loss: 0.6953\n",
      "98/156, train_loss: 0.6879\n",
      "99/156, train_loss: 0.6929\n",
      "100/156, train_loss: 0.6917\n",
      "101/156, train_loss: 0.6967\n",
      "102/156, train_loss: 0.6878\n",
      "103/156, train_loss: 0.6803\n",
      "104/156, train_loss: 0.6917\n",
      "105/156, train_loss: 0.6989\n",
      "106/156, train_loss: 0.6934\n",
      "107/156, train_loss: 0.6897\n",
      "108/156, train_loss: 0.6977\n",
      "109/156, train_loss: 0.6863\n",
      "110/156, train_loss: 0.6905\n",
      "111/156, train_loss: 0.6880\n",
      "112/156, train_loss: 0.6920\n",
      "113/156, train_loss: 0.6960\n",
      "114/156, train_loss: 0.6970\n",
      "115/156, train_loss: 0.6885\n",
      "116/156, train_loss: 0.6885\n",
      "117/156, train_loss: 0.6913\n",
      "118/156, train_loss: 0.6920\n",
      "119/156, train_loss: 0.6890\n",
      "120/156, train_loss: 0.6850\n",
      "121/156, train_loss: 0.6890\n",
      "122/156, train_loss: 0.6895\n",
      "123/156, train_loss: 0.6909\n",
      "124/156, train_loss: 0.6981\n",
      "125/156, train_loss: 0.6846\n",
      "126/156, train_loss: 0.6870\n",
      "127/156, train_loss: 0.6888\n",
      "128/156, train_loss: 0.7027\n",
      "129/156, train_loss: 0.7077\n",
      "130/156, train_loss: 0.6825\n",
      "131/156, train_loss: 0.6845\n",
      "132/156, train_loss: 0.6920\n",
      "133/156, train_loss: 0.6941\n",
      "134/156, train_loss: 0.6898\n",
      "135/156, train_loss: 0.6952\n",
      "136/156, train_loss: 0.6874\n",
      "137/156, train_loss: 0.6912\n",
      "138/156, train_loss: 0.6996\n",
      "139/156, train_loss: 0.6787\n",
      "140/156, train_loss: 0.6986\n",
      "141/156, train_loss: 0.6866\n",
      "142/156, train_loss: 0.6864\n",
      "143/156, train_loss: 0.6997\n",
      "144/156, train_loss: 0.6890\n",
      "145/156, train_loss: 0.6848\n",
      "146/156, train_loss: 0.6898\n",
      "147/156, train_loss: 0.6954\n",
      "148/156, train_loss: 0.6840\n",
      "149/156, train_loss: 0.6880\n",
      "150/156, train_loss: 0.6903\n",
      "151/156, train_loss: 0.6883\n",
      "152/156, train_loss: 0.6961\n",
      "153/156, train_loss: 0.6889\n",
      "154/156, train_loss: 0.6905\n",
      "155/156, train_loss: 0.6781\n",
      "156/156, train_loss: 0.6899\n",
      "157/156, train_loss: 0.6731\n",
      "epoch 1 average loss: 0.6920\n",
      "saved new best metric model\n",
      "current epoch: 1 current AUC: 0.5145 current accuracy: 0.5288 best AUC: 0.5145 at epoch: 1\n",
      "----------\n",
      "epoch 2/4\n",
      "1/156, train_loss: 0.6968\n",
      "2/156, train_loss: 0.6879\n",
      "3/156, train_loss: 0.7010\n",
      "4/156, train_loss: 0.6986\n",
      "5/156, train_loss: 0.6796\n",
      "6/156, train_loss: 0.6960\n",
      "7/156, train_loss: 0.6899\n",
      "8/156, train_loss: 0.6863\n",
      "9/156, train_loss: 0.6813\n",
      "10/156, train_loss: 0.6937\n",
      "11/156, train_loss: 0.6865\n",
      "12/156, train_loss: 0.6890\n",
      "13/156, train_loss: 0.6879\n",
      "14/156, train_loss: 0.6892\n",
      "15/156, train_loss: 0.7022\n",
      "16/156, train_loss: 0.6943\n",
      "17/156, train_loss: 0.6979\n",
      "18/156, train_loss: 0.6940\n",
      "19/156, train_loss: 0.6954\n",
      "20/156, train_loss: 0.6845\n",
      "21/156, train_loss: 0.6858\n",
      "22/156, train_loss: 0.6845\n",
      "23/156, train_loss: 0.6729\n",
      "24/156, train_loss: 0.6808\n",
      "25/156, train_loss: 0.6838\n",
      "26/156, train_loss: 0.6838\n",
      "27/156, train_loss: 0.7045\n",
      "28/156, train_loss: 0.6889\n",
      "29/156, train_loss: 0.6978\n",
      "30/156, train_loss: 0.6907\n",
      "31/156, train_loss: 0.6878\n",
      "32/156, train_loss: 0.7015\n",
      "33/156, train_loss: 0.6649\n",
      "34/156, train_loss: 0.6792\n",
      "35/156, train_loss: 0.6952\n",
      "36/156, train_loss: 0.6874\n",
      "37/156, train_loss: 0.6908\n",
      "38/156, train_loss: 0.6713\n",
      "39/156, train_loss: 0.6851\n",
      "40/156, train_loss: 0.6861\n",
      "41/156, train_loss: 0.6879\n",
      "42/156, train_loss: 0.6970\n",
      "43/156, train_loss: 0.6773\n",
      "44/156, train_loss: 0.6844\n",
      "45/156, train_loss: 0.6934\n",
      "46/156, train_loss: 0.6879\n",
      "47/156, train_loss: 0.6917\n",
      "48/156, train_loss: 0.6990\n",
      "49/156, train_loss: 0.6804\n",
      "50/156, train_loss: 0.6820\n",
      "51/156, train_loss: 0.6709\n",
      "52/156, train_loss: 0.7028\n",
      "53/156, train_loss: 0.6919\n",
      "54/156, train_loss: 0.6971\n",
      "55/156, train_loss: 0.6982\n",
      "56/156, train_loss: 0.6805\n",
      "57/156, train_loss: 0.6866\n",
      "58/156, train_loss: 0.7054\n",
      "59/156, train_loss: 0.6872\n",
      "60/156, train_loss: 0.6865\n",
      "61/156, train_loss: 0.6940\n",
      "62/156, train_loss: 0.6827\n",
      "63/156, train_loss: 0.6712\n",
      "64/156, train_loss: 0.6810\n",
      "65/156, train_loss: 0.6959\n",
      "66/156, train_loss: 0.6955\n",
      "67/156, train_loss: 0.6901\n",
      "68/156, train_loss: 0.6851\n",
      "69/156, train_loss: 0.6813\n",
      "70/156, train_loss: 0.6776\n",
      "71/156, train_loss: 0.6808\n",
      "72/156, train_loss: 0.6847\n",
      "73/156, train_loss: 0.6856\n",
      "74/156, train_loss: 0.7017\n",
      "75/156, train_loss: 0.7062\n",
      "76/156, train_loss: 0.6962\n",
      "77/156, train_loss: 0.6933\n",
      "78/156, train_loss: 0.6857\n",
      "79/156, train_loss: 0.6855\n",
      "80/156, train_loss: 0.7094\n",
      "81/156, train_loss: 0.6773\n",
      "82/156, train_loss: 0.6946\n",
      "83/156, train_loss: 0.7032\n",
      "84/156, train_loss: 0.6870\n",
      "85/156, train_loss: 0.6775\n",
      "86/156, train_loss: 0.7069\n",
      "87/156, train_loss: 0.6940\n",
      "88/156, train_loss: 0.6910\n",
      "89/156, train_loss: 0.6886\n",
      "90/156, train_loss: 0.6937\n",
      "91/156, train_loss: 0.6830\n",
      "92/156, train_loss: 0.6808\n",
      "93/156, train_loss: 0.6965\n",
      "94/156, train_loss: 0.6789\n",
      "95/156, train_loss: 0.7061\n",
      "96/156, train_loss: 0.6956\n",
      "97/156, train_loss: 0.6775\n",
      "98/156, train_loss: 0.6708\n",
      "99/156, train_loss: 0.7065\n",
      "100/156, train_loss: 0.7037\n",
      "101/156, train_loss: 0.6854\n",
      "102/156, train_loss: 0.6855\n",
      "103/156, train_loss: 0.7012\n",
      "104/156, train_loss: 0.6831\n",
      "105/156, train_loss: 0.6957\n",
      "106/156, train_loss: 0.6881\n",
      "107/156, train_loss: 0.6852\n",
      "108/156, train_loss: 0.6913\n",
      "109/156, train_loss: 0.6854\n",
      "110/156, train_loss: 0.6911\n",
      "111/156, train_loss: 0.6968\n",
      "112/156, train_loss: 0.6771\n",
      "113/156, train_loss: 0.6885\n",
      "114/156, train_loss: 0.6947\n",
      "115/156, train_loss: 0.6857\n",
      "116/156, train_loss: 0.6764\n",
      "117/156, train_loss: 0.6978\n",
      "118/156, train_loss: 0.6941\n",
      "119/156, train_loss: 0.6839\n",
      "120/156, train_loss: 0.6870\n",
      "121/156, train_loss: 0.6842\n",
      "122/156, train_loss: 0.6919\n",
      "123/156, train_loss: 0.6854\n",
      "124/156, train_loss: 0.6983\n",
      "125/156, train_loss: 0.6860\n",
      "126/156, train_loss: 0.6842\n",
      "127/156, train_loss: 0.6959\n",
      "128/156, train_loss: 0.6853\n",
      "129/156, train_loss: 0.6864\n",
      "130/156, train_loss: 0.6896\n",
      "131/156, train_loss: 0.6911\n",
      "132/156, train_loss: 0.6891\n",
      "133/156, train_loss: 0.6829\n",
      "134/156, train_loss: 0.6826\n",
      "135/156, train_loss: 0.6862\n",
      "136/156, train_loss: 0.7024\n",
      "137/156, train_loss: 0.6926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/156, train_loss: 0.6738\n",
      "139/156, train_loss: 0.6853\n",
      "140/156, train_loss: 0.6880\n",
      "141/156, train_loss: 0.6965\n",
      "142/156, train_loss: 0.6813\n",
      "143/156, train_loss: 0.6989\n",
      "144/156, train_loss: 0.7113\n",
      "145/156, train_loss: 0.6951\n",
      "146/156, train_loss: 0.6796\n",
      "147/156, train_loss: 0.6854\n",
      "148/156, train_loss: 0.7024\n",
      "149/156, train_loss: 0.6850\n",
      "150/156, train_loss: 0.7075\n",
      "151/156, train_loss: 0.7081\n",
      "152/156, train_loss: 0.6854\n",
      "153/156, train_loss: 0.6944\n",
      "154/156, train_loss: 0.6935\n",
      "155/156, train_loss: 0.6772\n",
      "156/156, train_loss: 0.6955\n",
      "157/156, train_loss: 0.7343\n",
      "epoch 2 average loss: 0.6899\n",
      "saved new best metric model\n",
      "current epoch: 2 current AUC: 0.5325 current accuracy: 0.5377 best AUC: 0.5325 at epoch: 2\n",
      "----------\n",
      "epoch 3/4\n",
      "1/156, train_loss: 0.6890\n",
      "2/156, train_loss: 0.6888\n",
      "3/156, train_loss: 0.6899\n",
      "4/156, train_loss: 0.6896\n",
      "5/156, train_loss: 0.6742\n",
      "6/156, train_loss: 0.6869\n",
      "7/156, train_loss: 0.6916\n",
      "8/156, train_loss: 0.6771\n",
      "9/156, train_loss: 0.6838\n",
      "10/156, train_loss: 0.6893\n",
      "11/156, train_loss: 0.6840\n",
      "12/156, train_loss: 0.6917\n",
      "13/156, train_loss: 0.6957\n",
      "14/156, train_loss: 0.6924\n",
      "15/156, train_loss: 0.6830\n",
      "16/156, train_loss: 0.6819\n",
      "17/156, train_loss: 0.6767\n",
      "18/156, train_loss: 0.6727\n",
      "19/156, train_loss: 0.6873\n",
      "20/156, train_loss: 0.6926\n",
      "21/156, train_loss: 0.6921\n",
      "22/156, train_loss: 0.6719\n",
      "23/156, train_loss: 0.6944\n",
      "24/156, train_loss: 0.6714\n",
      "25/156, train_loss: 0.6737\n",
      "26/156, train_loss: 0.6837\n",
      "27/156, train_loss: 0.6800\n",
      "28/156, train_loss: 0.6936\n",
      "29/156, train_loss: 0.6956\n",
      "30/156, train_loss: 0.6762\n",
      "31/156, train_loss: 0.6927\n",
      "32/156, train_loss: 0.6999\n",
      "33/156, train_loss: 0.6927\n",
      "34/156, train_loss: 0.6881\n",
      "35/156, train_loss: 0.7029\n",
      "36/156, train_loss: 0.7050\n",
      "37/156, train_loss: 0.6964\n",
      "38/156, train_loss: 0.6837\n",
      "39/156, train_loss: 0.6845\n",
      "40/156, train_loss: 0.6857\n",
      "41/156, train_loss: 0.6920\n",
      "42/156, train_loss: 0.6820\n",
      "43/156, train_loss: 0.6932\n",
      "44/156, train_loss: 0.6818\n",
      "45/156, train_loss: 0.6819\n",
      "46/156, train_loss: 0.6954\n",
      "47/156, train_loss: 0.6724\n",
      "48/156, train_loss: 0.7045\n",
      "49/156, train_loss: 0.7057\n",
      "50/156, train_loss: 0.6861\n",
      "51/156, train_loss: 0.6880\n",
      "52/156, train_loss: 0.6705\n",
      "53/156, train_loss: 0.6933\n",
      "54/156, train_loss: 0.6911\n",
      "55/156, train_loss: 0.6858\n",
      "56/156, train_loss: 0.6803\n",
      "57/156, train_loss: 0.6944\n",
      "58/156, train_loss: 0.6816\n",
      "59/156, train_loss: 0.6917\n",
      "60/156, train_loss: 0.6746\n",
      "61/156, train_loss: 0.6804\n",
      "62/156, train_loss: 0.6970\n",
      "63/156, train_loss: 0.6867\n",
      "64/156, train_loss: 0.6994\n",
      "65/156, train_loss: 0.6704\n",
      "66/156, train_loss: 0.6881\n",
      "67/156, train_loss: 0.6862\n",
      "68/156, train_loss: 0.6804\n",
      "69/156, train_loss: 0.6836\n",
      "70/156, train_loss: 0.7014\n",
      "71/156, train_loss: 0.6970\n",
      "72/156, train_loss: 0.6933\n",
      "73/156, train_loss: 0.6925\n",
      "74/156, train_loss: 0.6889\n",
      "75/156, train_loss: 0.6928\n",
      "76/156, train_loss: 0.6749\n",
      "77/156, train_loss: 0.6894\n",
      "78/156, train_loss: 0.6889\n",
      "79/156, train_loss: 0.6995\n",
      "80/156, train_loss: 0.6682\n",
      "81/156, train_loss: 0.6900\n",
      "82/156, train_loss: 0.6917\n",
      "83/156, train_loss: 0.6912\n",
      "84/156, train_loss: 0.6817\n",
      "85/156, train_loss: 0.6711\n",
      "86/156, train_loss: 0.6863\n",
      "87/156, train_loss: 0.6784\n",
      "88/156, train_loss: 0.7001\n",
      "89/156, train_loss: 0.6686\n",
      "90/156, train_loss: 0.6868\n",
      "91/156, train_loss: 0.6866\n",
      "92/156, train_loss: 0.6792\n",
      "93/156, train_loss: 0.6894\n",
      "94/156, train_loss: 0.6680\n",
      "95/156, train_loss: 0.6716\n",
      "96/156, train_loss: 0.6886\n",
      "97/156, train_loss: 0.6895\n",
      "98/156, train_loss: 0.6921\n",
      "99/156, train_loss: 0.6879\n",
      "100/156, train_loss: 0.6942\n",
      "101/156, train_loss: 0.6810\n",
      "102/156, train_loss: 0.6943\n",
      "103/156, train_loss: 0.6861\n",
      "104/156, train_loss: 0.6792\n",
      "105/156, train_loss: 0.7000\n",
      "106/156, train_loss: 0.7044\n",
      "107/156, train_loss: 0.6708\n",
      "108/156, train_loss: 0.6873\n",
      "109/156, train_loss: 0.6981\n",
      "110/156, train_loss: 0.7131\n",
      "111/156, train_loss: 0.6832\n",
      "112/156, train_loss: 0.6955\n",
      "113/156, train_loss: 0.6868\n",
      "114/156, train_loss: 0.7162\n",
      "115/156, train_loss: 0.6972\n",
      "116/156, train_loss: 0.7002\n",
      "117/156, train_loss: 0.6898\n",
      "118/156, train_loss: 0.6951\n",
      "119/156, train_loss: 0.7067\n",
      "120/156, train_loss: 0.6929\n",
      "121/156, train_loss: 0.6924\n",
      "122/156, train_loss: 0.6970\n",
      "123/156, train_loss: 0.6973\n",
      "124/156, train_loss: 0.6834\n",
      "125/156, train_loss: 0.6962\n",
      "126/156, train_loss: 0.6712\n",
      "127/156, train_loss: 0.7020\n",
      "128/156, train_loss: 0.6865\n",
      "129/156, train_loss: 0.6751\n",
      "130/156, train_loss: 0.6807\n",
      "131/156, train_loss: 0.6827\n",
      "132/156, train_loss: 0.6897\n",
      "133/156, train_loss: 0.6783\n",
      "134/156, train_loss: 0.6779\n",
      "135/156, train_loss: 0.6771\n",
      "136/156, train_loss: 0.6859\n",
      "137/156, train_loss: 0.6784\n",
      "138/156, train_loss: 0.6704\n",
      "139/156, train_loss: 0.6956\n",
      "140/156, train_loss: 0.6823\n",
      "141/156, train_loss: 0.6730\n",
      "142/156, train_loss: 0.6931\n",
      "143/156, train_loss: 0.6898\n",
      "144/156, train_loss: 0.6853\n",
      "145/156, train_loss: 0.7011\n",
      "146/156, train_loss: 0.6869\n",
      "147/156, train_loss: 0.6651\n",
      "148/156, train_loss: 0.7086\n",
      "149/156, train_loss: 0.6768\n",
      "150/156, train_loss: 0.7015\n",
      "151/156, train_loss: 0.6828\n",
      "152/156, train_loss: 0.6860\n",
      "153/156, train_loss: 0.7005\n",
      "154/156, train_loss: 0.6945\n",
      "155/156, train_loss: 0.7010\n",
      "156/156, train_loss: 0.6841\n",
      "157/156, train_loss: 0.6979\n",
      "epoch 3 average loss: 0.6878\n",
      "current epoch: 3 current AUC: 0.5240 current accuracy: 0.5160 best AUC: 0.5325 at epoch: 2\n",
      "----------\n",
      "epoch 4/4\n",
      "1/156, train_loss: 0.6766\n",
      "2/156, train_loss: 0.7092\n",
      "3/156, train_loss: 0.6896\n",
      "4/156, train_loss: 0.6832\n",
      "5/156, train_loss: 0.6937\n",
      "6/156, train_loss: 0.6932\n",
      "7/156, train_loss: 0.6748\n",
      "8/156, train_loss: 0.7005\n",
      "9/156, train_loss: 0.6882\n",
      "10/156, train_loss: 0.6841\n",
      "11/156, train_loss: 0.6933\n",
      "12/156, train_loss: 0.6846\n",
      "13/156, train_loss: 0.6619\n",
      "14/156, train_loss: 0.6952\n",
      "15/156, train_loss: 0.6741\n",
      "16/156, train_loss: 0.6726\n",
      "17/156, train_loss: 0.6753\n",
      "18/156, train_loss: 0.6815\n",
      "19/156, train_loss: 0.6869\n",
      "20/156, train_loss: 0.6843\n",
      "21/156, train_loss: 0.6819\n",
      "22/156, train_loss: 0.6910\n",
      "23/156, train_loss: 0.6923\n",
      "24/156, train_loss: 0.6943\n",
      "25/156, train_loss: 0.6864\n",
      "26/156, train_loss: 0.6872\n",
      "27/156, train_loss: 0.6993\n",
      "28/156, train_loss: 0.6927\n",
      "29/156, train_loss: 0.6818\n",
      "30/156, train_loss: 0.6948\n",
      "31/156, train_loss: 0.6758\n",
      "32/156, train_loss: 0.7028\n",
      "33/156, train_loss: 0.6808\n",
      "34/156, train_loss: 0.7124\n",
      "35/156, train_loss: 0.6894\n",
      "36/156, train_loss: 0.6855\n",
      "37/156, train_loss: 0.6878\n",
      "38/156, train_loss: 0.6836\n",
      "39/156, train_loss: 0.6789\n",
      "40/156, train_loss: 0.6938\n",
      "41/156, train_loss: 0.6903\n",
      "42/156, train_loss: 0.6769\n",
      "43/156, train_loss: 0.6873\n",
      "44/156, train_loss: 0.6916\n",
      "45/156, train_loss: 0.6764\n",
      "46/156, train_loss: 0.6895\n",
      "47/156, train_loss: 0.6808\n",
      "48/156, train_loss: 0.6852\n",
      "49/156, train_loss: 0.6793\n",
      "50/156, train_loss: 0.6857\n",
      "51/156, train_loss: 0.6833\n",
      "52/156, train_loss: 0.6888\n",
      "53/156, train_loss: 0.6968\n",
      "54/156, train_loss: 0.6689\n",
      "55/156, train_loss: 0.6807\n",
      "56/156, train_loss: 0.6737\n",
      "57/156, train_loss: 0.6761\n",
      "58/156, train_loss: 0.6751\n",
      "59/156, train_loss: 0.6732\n",
      "60/156, train_loss: 0.6921\n",
      "61/156, train_loss: 0.6671\n",
      "62/156, train_loss: 0.6796\n",
      "63/156, train_loss: 0.6933\n",
      "64/156, train_loss: 0.6784\n",
      "65/156, train_loss: 0.6769\n",
      "66/156, train_loss: 0.6698\n",
      "67/156, train_loss: 0.6529\n",
      "68/156, train_loss: 0.6779\n",
      "69/156, train_loss: 0.6794\n",
      "70/156, train_loss: 0.6767\n",
      "71/156, train_loss: 0.6802\n",
      "72/156, train_loss: 0.6962\n",
      "73/156, train_loss: 0.6792\n",
      "74/156, train_loss: 0.6658\n",
      "75/156, train_loss: 0.6807\n",
      "76/156, train_loss: 0.6971\n",
      "77/156, train_loss: 0.6542\n",
      "78/156, train_loss: 0.6902\n",
      "79/156, train_loss: 0.6775\n",
      "80/156, train_loss: 0.6953\n",
      "81/156, train_loss: 0.7117\n",
      "82/156, train_loss: 0.6954\n",
      "83/156, train_loss: 0.6929\n",
      "84/156, train_loss: 0.6937\n",
      "85/156, train_loss: 0.6838\n",
      "86/156, train_loss: 0.6912\n",
      "87/156, train_loss: 0.6779\n",
      "88/156, train_loss: 0.6932\n",
      "89/156, train_loss: 0.6935\n",
      "90/156, train_loss: 0.6753\n",
      "91/156, train_loss: 0.6981\n",
      "92/156, train_loss: 0.6738\n",
      "93/156, train_loss: 0.6950\n",
      "94/156, train_loss: 0.6958\n",
      "95/156, train_loss: 0.6947\n",
      "96/156, train_loss: 0.6881\n",
      "97/156, train_loss: 0.6817\n",
      "98/156, train_loss: 0.6973\n",
      "99/156, train_loss: 0.6785\n",
      "100/156, train_loss: 0.7002\n",
      "101/156, train_loss: 0.6991\n",
      "102/156, train_loss: 0.6787\n",
      "103/156, train_loss: 0.6766\n",
      "104/156, train_loss: 0.6976\n",
      "105/156, train_loss: 0.6749\n",
      "106/156, train_loss: 0.6754\n",
      "107/156, train_loss: 0.6757\n",
      "108/156, train_loss: 0.6844\n",
      "109/156, train_loss: 0.6888\n",
      "110/156, train_loss: 0.6925\n",
      "111/156, train_loss: 0.6907\n",
      "112/156, train_loss: 0.6653\n",
      "113/156, train_loss: 0.6831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/156, train_loss: 0.6855\n",
      "115/156, train_loss: 0.6860\n",
      "116/156, train_loss: 0.6510\n",
      "117/156, train_loss: 0.6715\n",
      "118/156, train_loss: 0.6814\n",
      "119/156, train_loss: 0.6720\n",
      "120/156, train_loss: 0.6923\n",
      "121/156, train_loss: 0.6800\n",
      "122/156, train_loss: 0.6724\n",
      "123/156, train_loss: 0.6860\n",
      "124/156, train_loss: 0.7003\n",
      "125/156, train_loss: 0.6473\n",
      "126/156, train_loss: 0.6790\n",
      "127/156, train_loss: 0.7075\n",
      "128/156, train_loss: 0.6883\n",
      "129/156, train_loss: 0.6752\n",
      "130/156, train_loss: 0.6876\n",
      "131/156, train_loss: 0.6821\n",
      "132/156, train_loss: 0.6957\n",
      "133/156, train_loss: 0.6700\n",
      "134/156, train_loss: 0.6850\n",
      "135/156, train_loss: 0.6681\n",
      "136/156, train_loss: 0.6810\n",
      "137/156, train_loss: 0.6847\n",
      "138/156, train_loss: 0.6711\n",
      "139/156, train_loss: 0.6664\n",
      "140/156, train_loss: 0.6935\n",
      "141/156, train_loss: 0.6877\n",
      "142/156, train_loss: 0.6996\n",
      "143/156, train_loss: 0.6892\n",
      "144/156, train_loss: 0.6795\n",
      "145/156, train_loss: 0.6679\n",
      "146/156, train_loss: 0.6774\n",
      "147/156, train_loss: 0.6931\n",
      "148/156, train_loss: 0.6699\n",
      "149/156, train_loss: 0.6878\n",
      "150/156, train_loss: 0.6758\n",
      "151/156, train_loss: 0.7091\n",
      "152/156, train_loss: 0.6863\n",
      "153/156, train_loss: 0.6734\n",
      "154/156, train_loss: 0.6781\n",
      "155/156, train_loss: 0.6883\n",
      "156/156, train_loss: 0.6777\n",
      "157/156, train_loss: 0.7038\n",
      "epoch 4 average loss: 0.6842\n",
      "saved new best metric model\n",
      "current epoch: 4 current AUC: 0.5388 current accuracy: 0.5168 best AUC: 0.5388 at epoch: 4\n",
      "train completed, best_metric: 0.5388 at epoch: 4\n"
     ]
    }
   ],
   "source": [
    "# train model \n",
    "max_epochs = 4\n",
    "best_metric = -1\n",
    "val_interval = 1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model = model.float()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    # batch size of 64 -> 195 batches per epoch\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(training) // train_loader.batch_size}, \"\n",
    "            f\"train_loss: {loss.item():.4f}\")\n",
    "        epoch_len = len(training) // train_loader.batch_size\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "            y = torch.tensor([], dtype=torch.long, device=device)\n",
    "            for val_data in val_loader:\n",
    "                val_images, val_labels = (\n",
    "                    val_data[0].to(device),\n",
    "                    val_data[1].to(device),\n",
    "                )\n",
    "                y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n",
    "                y = torch.cat([y, val_labels], dim=0)\n",
    "            y_onehot = [y_trans(i) for i in decollate_batch(y)]\n",
    "            y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n",
    "            auc_metric(y_pred_act, y_onehot)\n",
    "            result = auc_metric.aggregate()\n",
    "            auc_metric.reset()\n",
    "            del y_pred_act, y_onehot\n",
    "            metric_values.append(result)\n",
    "            acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
    "            acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "            if result > best_metric:\n",
    "                best_metric = result\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), \"best_metric_model.pth\")\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current AUC: {result:.4f}\"\n",
    "                f\" current accuracy: {acc_metric:.4f}\"\n",
    "                f\" best AUC: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "\n",
    "print(\n",
    "    f\"train completed, best_metric: {best_metric:.4f} \"\n",
    "    f\"at epoch: {best_metric_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "c731c0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting loss\n",
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Average loss (cross entropy)\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "\n",
    "# plotting AUC\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"AUC on validation data\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim([0, 1])\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "c8fef860",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\n",
    "    os.path.join(\"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        test_images, test_labels = (\n",
    "            test_data[0].to(device),\n",
    "            test_data[1].to(device),\n",
    "        )\n",
    "        pred = model(test_images).argmax(dim=1)\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_pred.append(pred[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "a91060dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.86      0.65       650\n",
      "           1       0.48      0.14      0.22       598\n",
      "\n",
      "    accuracy                           0.52      1248\n",
      "   macro avg       0.50      0.50      0.43      1248\n",
      "weighted avg       0.50      0.52      0.44      1248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce2bd8",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "- In summary, success for GANSformer.\n",
    "- Between the precision scores at around .50 and a best AUC of 0.54, we see this classifier performs no better than chance. \n",
    "- That is, it is not able to discern between real CT scans and those generated by GANSformer.\n",
    "- We also see that the loss stays constant at ~0.68 from epoch 1 to 4 indicating no improvement over epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
